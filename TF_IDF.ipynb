{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50f89be4",
   "metadata": {},
   "source": [
    "### Проект для «Викишоп»"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5485241e",
   "metadata": {},
   "source": [
    "Интернет-магазин запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98ce9ac",
   "metadata": {},
   "source": [
    "Описание данных\n",
    "\n",
    "Данные находятся в файле toxic_comments.csv. Столбец text в нём содержит текст комментария, а toxic — целевой признак."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c9ee72",
   "metadata": {},
   "source": [
    "### 1 Подготовка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "308c41e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\aleks\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\aleks\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\aleks\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.utils import shuffle\n",
    "import nltk\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import torch\n",
    "import transformers\n",
    "from tqdm import notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ff9dac4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"\\n\\nCongratulations from me as well, use the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Your vandalism to the Matt Shirvington article...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sorry if the word 'nonsense' was offensive to ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>alignment on this subject and which are contra...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic\n",
       "0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1  D'aww! He matches this background colour I'm s...      0\n",
       "2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4  You, sir, are my hero. Any chance you remember...      0\n",
       "5  \"\\n\\nCongratulations from me as well, use the ...      0\n",
       "6       COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK      1\n",
       "7  Your vandalism to the Matt Shirvington article...      0\n",
       "8  Sorry if the word 'nonsense' was offensive to ...      0\n",
       "9  alignment on this subject and which are contra...      0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments = pd.read_csv('toxic_comments.csv')\n",
    "comments.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1f3fe6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159571 entries, 0 to 159570\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    159571 non-null  object\n",
      " 1   toxic   159571 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 2.4+ MB\n"
     ]
    }
   ],
   "source": [
    "comments.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3e92c9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    143346\n",
       "1     16225\n",
       "Name: toxic, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments['toxic'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82d6a227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Процент объектов класса 1 к общему объёму датасета: 10.17%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Процент объектов класса 1 к общему объёму датасета: {(sum(comments['toxic']) / len(comments) * 100):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75981f7d",
   "metadata": {},
   "source": [
    "Подготовим данные для векторизации.\n",
    "\n",
    " * Приведём кодировку символов к Unicode\n",
    " * Проведём лемматизацию слов с помощью WordNetLemmatizer() из библиотеки nltk\n",
    " * Удалим пунктуацию и лишние пробелы\n",
    " * Удалим стоп-слова (пока загрузим список, удалять будем в процессе tf-idf векторизации)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22350672",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize(text):\n",
    "    word_list = nltk.word_tokenize(text)\n",
    "    lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in word_list])\n",
    "    return lemmatized_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fff929e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_text(text):\n",
    "    text = re.sub(r'[^a-zA-Z ]', ' ', text)\n",
    "    return \" \".join(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b49477d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = comments['text'].values.astype('U')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2be5617",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_lemm = [lemmatize(clear_text(corpus[i])) for i in range(len(corpus))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9e38282",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(nltk_stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f429dc35",
   "metadata": {},
   "source": [
    "Разделим данные на тренировочную и тестовую выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "760ae280",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(corpus_lemm, comments['toxic'], \n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54c58ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер тренировочного корпуса: 127656\n",
      "Размер тестового корпуса: 31915\n"
     ]
    }
   ],
   "source": [
    "print(f\"Размер тренировочного корпуса: {len(X_train)}\")\n",
    "print(f\"Размер тестового корпуса: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600e841c",
   "metadata": {},
   "source": [
    "Проведём векторизацию корпусов с помощью TfidfVectorizer, заодно удалим стоп-слова.\n",
    "Попробуем обучить модель без использования n-gramm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d772075",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_vec = TfidfVectorizer(ngram_range=(1,1), stop_words=stopwords,\n",
    "               min_df=3, max_df=0.9, strip_accents='unicode', use_idf=1,\n",
    "               smooth_idf=1, sublinear_tf=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "407f4da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vec = tf_idf_vec.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2137c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_vec = tf_idf_vec.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b5e7993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер тренировочного датасета: (127656, 41830)\n",
      "Размер тренировочного датасета: (31915, 41830)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Размер тренировочного датасета: {X_train_vec.shape}\")\n",
    "print(f\"Размер тренировочного датасета: {X_test_vec.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934bc08f",
   "metadata": {},
   "source": [
    "### 2. Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ece1b7",
   "metadata": {},
   "source": [
    "Найдём метрику accuracy для константной модели. Будем предсказывать все твиты нетоксичными ('toxic'=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0fd55a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy константной модели 0.898\n"
     ]
    }
   ],
   "source": [
    "base_predicts = pd.Series(data=np.zeros((len(y_test))), index=y_test.index, dtype='int16')\n",
    "base_accuacy = accuracy_score(y_test, base_predicts)\n",
    "print(f\"Accuracy константной модели {base_accuacy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fccf03",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7dac7a9",
   "metadata": {},
   "source": [
    "Для начала попробуем обучить модель логистической регрессии.\n",
    "Обучение, подбор гиперпараметров, кросс-валидацию проведём с помощью GridSearchCV библиотеки sklearn\n",
    "Подбирать будем гиперпараметр регуляризации С"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e5378d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 11 candidates, totalling 55 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=LogisticRegression(), n_jobs=-1,\n",
       "             param_grid={'C': array([10., 11., 12., 13., 14., 15., 16., 17., 18., 19., 20.]),\n",
       "                         'max_iter': [1000]},\n",
       "             scoring='f1', verbose=2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {'C': np.linspace(10, 20, num = 11, endpoint = True),\n",
    "             'max_iter': [1000]}\n",
    "lrm = LogisticRegression()\n",
    "clf = GridSearchCV(lrm, parameters,\n",
    "                  cv=5,\n",
    "                  scoring='f1',\n",
    "                  n_jobs=-1,\n",
    "                  verbose=2)\n",
    "clf.fit(X_train_vec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b440cf3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Наилучший показатель f1 на кросс-валидации : 0.773\n",
      "Параметр регуляризации для лучшей модели: {'C': 13.0, 'max_iter': 1000}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Наилучший показатель f1 на кросс-валидации : {clf.best_score_:.3f}\")\n",
    "print(f\"Параметр регуляризации для лучшей модели: {clf.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e4c27a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrm = LogisticRegression(C=13, max_iter=1000)\n",
    "lrm.fit(X_train_vec, y_train)\n",
    "predict = lrm.predict(X_test_vec)\n",
    "f1_lr = f1_score(y_test, predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fe309a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Показатель f1 на тестовой выборке: 0.783\n"
     ]
    }
   ],
   "source": [
    "print(f\"Показатель f1 на тестовой выборке: {f1_lr:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4bd517",
   "metadata": {},
   "source": [
    " * Проверим модель на адекватность. Рассчитаем метрику accuracy и сравним её с константной моделью"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cb1e09a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy на логистической регрессии 0.960, больше, чем на константной модели\n"
     ]
    }
   ],
   "source": [
    "accuracy_lr = accuracy_score(y_test, predict)\n",
    "print(f\"Accuracy на логистической регрессии {accuracy_lr:.3f}, больше, чем на константной модели\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a9e98b",
   "metadata": {},
   "source": [
    "Показатель f1 на тестовой выборке удовлетворяет условию задачи."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f7531d",
   "metadata": {},
   "source": [
    "##### NB-SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2070a09c",
   "metadata": {},
   "source": [
    "Теперь попробуем модель NBSVM (Naive Bayes - Support Vector Machine). В данной задаче будем испльзовать модель LinearRegression с алгоритмом оптимизации solver='liblinear', dual=True. В таком случае эти модели ведут себя похожим образом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "af89051c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob(x, y_i, y):\n",
    "    p = x[y==y_i].sum(axis=0)\n",
    "    return (p+1) / ((y==y_i).sum()+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a2e0ddf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.log(prob(X_train_vec, 1, y_train.values) / prob(X_train_vec, 0, y_train.values))\n",
    "X_train_nb = X_train_vec.multiply(r)\n",
    "X_test_nb = X_test_vec.multiply(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "46dfef7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'C': np.linspace(1, 11, num = 11, endpoint = True)}\n",
    "nblrm = LogisticRegression(solver='liblinear', \n",
    "                           dual=True, \n",
    "                           max_iter = 1000)\n",
    "clf_nb = GridSearchCV(nblrm, parameters,\n",
    "                  cv=5,\n",
    "                  scoring='f1',\n",
    "                  n_jobs=-1,\n",
    "                  verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5cfecb94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 11 candidates, totalling 55 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=LogisticRegression(dual=True, max_iter=1000,\n",
       "                                          solver='liblinear'),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'C': array([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.])},\n",
       "             scoring='f1', verbose=2)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_nb.fit(X_train_nb, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ec014c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Наилучший показатель f1 на кросс-валидации : 0.791\n",
      "Параметр регуляризации для лучшей модели: {'C': 3.0}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Наилучший показатель f1 на кросс-валидации : {clf_nb.best_score_:.3f}\")\n",
    "print(f\"Параметр регуляризации для лучшей модели: {clf_nb.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "18fc606a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nblrm = LogisticRegression(C=3,\n",
    "                           solver='liblinear', \n",
    "                           dual=True,\n",
    "                           max_iter=1000)\n",
    "nblrm.fit(X_train_nb, y_train)\n",
    "predict = nblrm.predict(X_test_nb)\n",
    "f1_nblr = f1_score(y_test, predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "91eb4595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Показатель f1 на тестовой выборке: 0.793\n"
     ]
    }
   ],
   "source": [
    "print(f\"Показатель f1 на тестовой выборке: {f1_nblr:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03736d19",
   "metadata": {},
   "source": [
    "Данная модель мало отличается от изначальной Логистической регрессии (f1 вырос на 1%)\n",
    "Для сравнения попробуем обучить модель LinearSVC. Linear Support Vector Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccabc50b",
   "metadata": {},
   "source": [
    "#### LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a48448a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'C': np.linspace(1, 31, num = 7, endpoint = True)}\n",
    "lsvcm = LinearSVC(max_iter = 1000)\n",
    "clf_lsvc = GridSearchCV(nblrm, parameters,\n",
    "                  cv=5,\n",
    "                  scoring='f1',\n",
    "                  n_jobs=-1,\n",
    "                  verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d5747add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=LogisticRegression(C=3, dual=True, max_iter=1000,\n",
       "                                          solver='liblinear'),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'C': array([ 1.,  6., 11., 16., 21., 26., 31.])},\n",
       "             scoring='f1', verbose=2)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_lsvc.fit(X_train_vec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2c12c524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Наилучший показатель f1 на кросс-валидации : 0.772\n",
      "Параметр регуляризации для лучшей модели: {'C': 11.0}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Наилучший показатель f1 на кросс-валидации : {clf_lsvc.best_score_:.3f}\")\n",
    "print(f\"Параметр регуляризации для лучшей модели: {clf_lsvc.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2b07fba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lsvcm = LogisticRegression(C=11,\n",
    "                           max_iter=1000)\n",
    "lsvcm.fit(X_train_vec, y_train)\n",
    "predict = lsvcm.predict(X_test_vec)\n",
    "f1_lsvc = f1_score(y_test, predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cabbded1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Показатель f1 на тестовой выборке: 0.782\n"
     ]
    }
   ],
   "source": [
    "print(f\"Показатель f1 на тестовой выборке: {f1_lsvc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0fc19d",
   "metadata": {},
   "source": [
    "### 3. Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79a2265",
   "metadata": {},
   "source": [
    "Данные о токсичности твитов успешно загружены и обработаны:\n",
    "Лемматизация проведена с помощью WordNetLemmatizer библиотеки nltk\n",
    "знаки пунктуации, а также лишние пробелы удалены\n",
    "Стоп слова удалены (список взят из библиотеки nltk)\n",
    "Корпус векторизован с помощью TfidfVectorizer\n",
    "На получившихся данных обучены модели: LogisticRegression, NB-SVM, LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "681df8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "report = pd.DataFrame(columns = [\"F1_score\"],\n",
    "                      index = [\"LogisticRegression\",\n",
    "                               \"NB-SVM\",\n",
    "                               \"LinearSVC\"])\n",
    "\n",
    "\n",
    "report.iloc[0] = [f1_lr]\n",
    "report.iloc[1] = [f1_nblr]\n",
    "report.iloc[2] = [f1_lsvc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3d3ce73b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LogisticRegression</th>\n",
       "      <td>0.782549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NB-SVM</th>\n",
       "      <td>0.79257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearSVC</th>\n",
       "      <td>0.781862</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    F1_score\n",
       "LogisticRegression  0.782549\n",
       "NB-SVM               0.79257\n",
       "LinearSVC           0.781862"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1af731",
   "metadata": {},
   "source": [
    "Качество моделей практически одинаково. Разница не более 1%. Максимальный показатель f1 получен для NB-SVM: 0.793\n",
    "Кросс-валидация моделей и подбор гиперпараметров проводились с помощью GridSearchCV.\n",
    "Проверка на адекватность была проведена для модели LogisticRegression."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
